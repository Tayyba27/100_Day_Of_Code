{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day_53\n",
    "### Web Scraping and Data Entry Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.11.5' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Tested with the following package versions:\n",
    "# beautifulsoup4==4.12.2\n",
    "# Requests==2.31.0\n",
    "# selenium==4.15.1\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Part 1 - Scrape the links, addresses, and prices of the rental properties\n",
    "\n",
    "header = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "}\n",
    "\n",
    "# Use our Zillow-Clone website (instead of Zillow.com)\n",
    "response = requests.get(\"https://appbrewery.github.io/Zillow-Clone/\", headers=header)\n",
    "\n",
    "data = response.text\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "# Create a list of all the links on the page using a CSS Selector\n",
    "all_link_elements = soup.select(\".StyledPropertyCardDataWrapper a\") \n",
    "# Python list comprehension (covered in Day 26)\n",
    "all_links = [link[\"href\"] for link in all_link_elements] \n",
    "print(f\"There are {len(all_links)} links to individual listings in total: \\n\")\n",
    "print(all_links)\n",
    "\n",
    "# Create a list of all the addresses on the page using a CSS Selector\n",
    "# Remove newlines \\n, pipe symbols |, and whitespaces to clean up the address data\n",
    "all_address_elements = soup.select(\".StyledPropertyCardDataWrapper address\")\n",
    "all_addresses = [address.get_text().replace(\" | \", \" \").strip() for address in all_address_elements]\n",
    "print(f\"\\n After having been cleaned up, the {len(all_addresses)} addresses now look like this: \\n\")\n",
    "print(all_addresses)\n",
    "\n",
    "# Create a list of all the prices on the page using a CSS Selector\n",
    "# Get a clean dollar price and strip off any \"+\" symbols and \"per month\" /mo abbreviation\n",
    "all_price_elements = soup.select(\".PropertyCardWrapper span\")\n",
    "all_prices = [price.get_text().replace(\"/mo\", \"\").split(\"+\")[0] for price in all_price_elements if \"$\" in price.text]\n",
    "print(f\"\\n After having been cleaned up, the {len(all_prices)} prices now look like this: \\n\")\n",
    "print(all_prices)\n",
    "\n",
    "\n",
    "# Part 2 - Fill in the Google Form using Selenium\n",
    "\n",
    "# Optional - Keep the browser open (helps diagnose issues if the script crashes)\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "for n in range(len(all_links)):\n",
    "    # TODO: Add fill in the link to your own Google From\n",
    "    driver.get(\"YOUR_GOOGLE_FORM_LINK_HERE\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Use the xpath to select the \"short answer\" fields in your Google Form. \n",
    "    # Note, your xpath might be different if you created a different form.\n",
    "    address = driver.find_element(by=By.XPATH, \n",
    "                        value='//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[1]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    price = driver.find_element(by=By.XPATH, \n",
    "                        value='//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[2]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    link = driver.find_element(by=By.XPATH, \n",
    "                        value='//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[3]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    submit_button = driver.find_element(by=By.XPATH, \n",
    "                        value='//*[@id=\"mG61Hd\"]/div[2]/div/div[3]/div[1]/div[1]/div')\n",
    "    \n",
    "    address.send_keys(all_addresses[n])\n",
    "    price.send_keys(all_prices[n])\n",
    "    link.send_keys(all_links[n])\n",
    "    submit_button.click()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
